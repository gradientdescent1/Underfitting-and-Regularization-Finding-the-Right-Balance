---
layout: distill
title: NETWORK AUGMENTATION FOR TINY DEEP LEARNING
description: In this blog post, we will go over the ICLR 2022 paper titled NETWORK AUGMENTATION FOR TINY DEEP LEARNING. This paper introduces a new training method for improving the performance of tiny neural networks. NetAug augments the network (reverse dropout), it puts the tiny model into larger models and encourages it to work as a sub-model of larger models to get extra supervision.
date: 2023-01-15
htmlwidgets: true

# Anonymize when submitting
authors:
  - name: Anonymous

# authors:
#   - name: Nevasini Sasikumar
#     url: "https://nevasini24.wixsite.com/my-site-1"
#     affiliations:
#       name: Department of Computer Science, PES University
#   - name: Krishna Sri Ipsit Mantri
#     url: "https://ipsitmantri.github.io/"
#     # affiliations:
#     #   name: IAS, Princeton
 

# must be the exact same name as your blogpost
bibliography: 

# Add a table of contents to your post.
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
toc:
  - name: Equations
  - name: Images and Figures
    subsections:
    - name: Interactive Figures
  - name: Citations
  - name: Footnotes
  - name: Code Blocks
  - name: Layouts
  - name: Other Typography?

# Below is an example of injecting additional post-specific styles.
# This is used in the 'Layouts' section of this post.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

## Goal of this blog post  
NetAug caters on how to train a small neural networks like MobileNetV2-Tiny for the best top-k percent accuracy that technically differs from training large neural networks, the techniques mentioned are contrary to traditional methods like network pruning,quantization and other regularization techniques. It can be viewed as a reversed form of dropout, as we enlarge the target model instead of shrinking it. 

## Background  
NetAug solely focuses on how to efficiently train small neural networks with less memory overhead and prevent underfitting. It's certainly easy to train a huge neural network the one most predominant issue that big models suffer from is overfitting several techniques like data augmentation,pruning,dropout,knowledge distillation have been proposed. 

1. **Knowledge Distillation**:  It is difficult to deploy and maintain an ensemble. However, previous research has shown that it is possible to learn a single cumbersome model that has the same performance as an ensemble. It is In most of these knowledge distillation methods there exist a large teacher model that transfers its knowledge via training a small student model. 

2. **Regularization**: Regularization is used to prevent overfitting of any ML model by reducing the variance, model coefficient size and complexity of the model. Regularization techniques are mainly composed of data augmentation one of the most simplest and conveninet ways to expand the size of the dataset such that it prevents overfitting issues that occur with a relatively small dataset. Dropout is also popularly applied while training models, in which at every iteration connection between nodes are randomly dropped based on a particular probability and the remaining neural network is trained normally. 

3. **Tiny Deep learning**: TinyML revolves around how effectively we deploy machine learning models without disturbing its accuracy into small and inexpensive edge devices, 

4. **Network Augmentation**: This method was proposed by Cai et al. idkkkkkk

---

## Network Augmentation a.k.a NetAug  


## The Problem With NetAug

1. **Random Selection of Augmented Models**: NetAug randomly samples any sub-model at every training iteration without paying attention to which sample is most effective in providing auxiliary supervision to the base model. In this scenario, even those sub-models that are poor in representations are trained that might lead to increase in unnecessary parameters that can severely waste memory space. It's very important to choose which specific sub-model is to be trained at every iteration rather than choosing completely at random.

2. **Naive Loss Function**: NetAug computes loss in a very trivial form, i.e, by simply summing over the loss of the base model with that from the respective sampled augmented models. The overall objective of any optimization is to find network such that it can maintain a tradeoffs between speed and accuracy, generalize to many diverse tasks and datasets. To make continued steady progress toward this objective, we must use the appropriate loss function, for comparing across varied network architectures that can tell us on how reliable the proposed method is.

3. **Overfitting of the Augmented Models**: As the network grows lineary even the number of augmented models increase, thereby overfitting during the training process. Having large number of such models with a poor representation among them will simply increase the training overhead and will cause haphazard confusion while selecting the largest augmented model. NetAug focuses on design and implementing a single best largest, going ahead with just the largest paramater model would yield in bad accuracy and will worsen the overall model performance.

4. **Generating the Augmented Model**: For a particular network, rather than varying a single network hyperparameter(ie: network
depth,width), what if instead we vary all relevant network hyperparameters for every augmented sub-model? To advocate this it's sensible to compare the entire distribution of hyperparameter across the model. 
---

## Formulation of NetAug

The end goal of any ML model is to be able to minimize the loss function with the help of gradient descent. Since Tiny neural network have a very small capacity the gradient descent is likely to get stuck in local minima. NetAug encourages the tiny neural network to work as a sub-model of a set of larger models constructed by augmenting the width of the tiny model. The largest augmented model,is constructed by augmenting the width ie: width of the largest augmented convolution operation is $$r \times w$$, where $$w$$ is width of a convolution operation and $$r$$ is augmentation factor.
After building the largest augmented model, we construct other augmented models by selecting a subset of channels from the largest augmented model. We use a hyper-parameter s, named diversity factor, to control the number of augmented model configurations. We set the augmented widths to be linearly spaced between w and r × w. For instance, with r = 3 and s = 2, the possible widths are [w, 2w, 3w].

### Introducing NetAug with Relation Knowledge Distribution(RKD): 

Knowledge in learned models is constituted of:  
    1. (IKD) Outputs of individual samples represented by the teacher  
    2. (RKD) Relation among examples represented by the teacher
    

  1. RKD is a generalization of convectional knowledge distillation that combines with NetAug to boost the performance due to its complementarity with conventional KD, that aims at transferring structural knowledge using mutual relations of data examples in the teacher’s output presentation rather than individual output themselves. Contrary to conventional approaches called as Individual KD (IKD) that transfers individual outputs of the teacher model (fT) to the student model (fS) point-wise, RKD transfers relations of the outputs structure-wise and computes a relational potential ψ for every n-tuple of data instance and transfers the relevant information through the potential from the teacher to the student models. In addition to knowledge represented in the output layers and the intermediate layers of a neural network, knowledge that captures the relationship between feature maps are also used to train a student model. 
 
  2. We specifically aim at training the teacher and student model in a online setting, in this online type of distillation training method/scheme is one in which both the teacher and the student model is trained simultaneously at the same instance of time. 

  Relational Knowledge distillation can be expressed by:

    - $$\chi$$: function extracting relation
      Given a teacher model T and a student model S, we let f(T) and f(S) be functions of the teacher and the student, respectively

  $$\mathcal{L}_{\text{RKD}} = \sum \limits_{\{x_1, \ldots, x_n\} \in \chi^N} l \big(\psi(t_1,\cdots,t_n), \: \psi(s_1, \cdots, s_n)\big)$$

  ## RKD Loss

  1. Distance-wise distillation loss(pair):

          RKD-D transfer relative distance between points on embedding space.

            $$\psi_{D}(t_i, t_j) = \frac{1}{\mu}\big\| t_i - t_j\big\|_2$$  

          $$\mu = \frac{1}{|\chi^2|}\sum\limits_{(x_i, x_j) \in \chi^2} \big\| t_i - t_j\big\|_2$$

          $$\mathcal{L}_{\text{RKD-D}} = \sum \limits_{(x_i, x_j) \in \chi^2} l_\delta \big(\psi_D(t_i, t_j), \psi_D(s_i, s_j)\big)$$

          $$l_\delta(x, y) = \begin{cases}
                              \frac{1}{2} (x-y)^2\:\:\:\: \text{for} |x-y|^2 
                              \end{cases}$$


      2. Angle-wise distillation loss(triplet):
       
          RKD-A transfers angle formed by three points on embedding space. 

          FILL EQUATION
---

### Combining RKD with NetAug:
    
$$\mathcal{L}_{\text{aug}} = \underbrace{\mathcal{L}(W_t)}_{\text{base supervision}} \:+\: \underbrace{\alpha_1 \mathcal{L}([W_t, W_1]) + \cdots + \alpha_i \mathcal{L}([W_t, W_i]) + \cdots}_{\text{auxiliary supervision, working as a sub-model of augmented models}}  \:+\: \lambda\,\underbrace{\mathbf{\mathcal{L}_{\text{RKD}}}}_{\text{relational knowledge distillation}}$$

where $$[W_t, W_i]$$ represents an augmented model where $$[W_t]$$ contains weight of the tiny neural network and $$[W_i]$$ contains weight of the new augmented model and $$\alpha$$ is scaling hyper-parameter for combining loss from different augmented models.

## Training

We train only one model for every epoch, training all the augmented models all once is not only computatioanlly expensive but also impacts the performance. The proportion of the base supervision will decrease when we sample more augmented networks, which will make the training process biased toward augmented networks and shadows the base TinyML model.

    

   ## PLS FILL THAT EQUATION

---

## Evaluation and Inference
  
































<!-- Notes/Overview/Category of the Paper, comment out later -->
### Category
Solves a known problem of underfitting by proposing a new method

### Context
Regularization, over-fitting, underfitting, data augmentation, model augmentation, training paradigm, TinyML, knowledge distallation, security, privacy

### Correctness
Assumptions appear to be valid, but there are few setbacks (like eq 1 of the paper)

### Contributions
NetAug, Ablation study, improving the performance of underfitting models, new ways to consider loss functions

### Clarity
Well-written overall. Somethings are not clear

<!-- Questions/clarifications asked by us -->
```markdown
Hi All, 

I hope everyone is doing well. I'm pursuing a BS from India and my friend is an employee. We came across this paper Network Augmentation for Tiny Deep Learning that was published in ICLR 22. We are unclear about a few things, would be great if you can clarify/justify them to us.

1. Why does sampling all augmented networks in one training step decrease the performance, given the training loss increases as one of the reasons?
2. Initially it's mentioned that NetAug is very different from the other knowledge distillation methods later the results show both NetAug and knowledge distillation combined together perform well, we would like to understand it more mathematically so as to why?
3. Things like data loading and communication cost are justified to consume maximum time, but don't we think we can spend training in doing something more potential like maybe in fact learning the feature maps/representations better?

Regards,
Nevasini.
```

```markdown
Hello Han Cai and team,
We are eager to get the above points clarified. Specifically, if you can answer along the following lines, it would be really helpful.
For 1 above: It was mentioned in the paper that it was found sampling more than one augmented network from the maintained single largest augmented model in each training step hurts the performance. We think this counters the intuition and spirit behind the idea of NetAug and the augmented loss proposed in equation (1) of the paper.
For 2 above: Can you further provide insights on why KD and NetAug can be complementary when the underlying motive in NetAug is to solve underfitting and in KD is to generalize well on unseem data with the help of a teacher? These both motives look like they want to solve the generalizability of the model under consideration. Furthermore, KD can be done to tiny models as well right?
For 3 above: We understand that data loading and communication c0st can be a bottleneck in training tiny neural networks. But in the case of NetAug, along with base supervision, we are training the augmented model as well (meaning we update the weights W_i). This means the training cost is expected to increase by 50% and experiments should also show a similar change.

Thanks and regards,
Nevasini and Ipsit.
```

```markdown
<!-- Their reply -->
Hi Nevasini and Ipsit,

 

1. I think it is because the proportion of the base supervision will decrease when we sample more augmented networks, which will make the training process biased toward augmented networks.

 

2. I think the mechanisms of KD and NetAug are different. KD can be viewed as a better training objective with adaptive label smoothing. It does not encourage the base network to work as a subnetwork of larger networks, different from NetAug.

 

3. For tiny neural networks, the cost of the additional forward+backward is smaller than you expected. For example, training MobileNetV2-Tiny with NetAug on ImageNet only increases the training time by 16.7% on our GPU servers.

 

Best,

Han
```

