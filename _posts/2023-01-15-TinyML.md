---
layout: distill
title: NETWORK AUGMENTATION FOR TINY DEEP LEARNING
description: In this blog post, we will go over the ICLR 2022 paper titled NETWORK AUGMENTATION FOR TINY DEEP LEARNING. This paper introduces a new training method for improving the performance of tiny neural networks. NetAug augments the network (reverse dropout), it puts the tiny model into larger models and encourages it to work as a sub-model of larger models to get extra supervision.
date: 2023-01-15
htmlwidgets: true

# Anonymize when submitting
authors:
  - name: Anonymous

# authors:
#   - name: Nevasini Sasikumar
#     url: "https://nevasini24.wixsite.com/my-site-1"
#     affiliations:
#       name: Department of Computer Science, PES University
#   - name: Krishna Sri Ipsit Mantri
#     url: "https://ipsitmantri.github.io/"
#     # affiliations:
#     #   name: IAS, Princeton
 

# must be the exact same name as your blogpost
bibliography: 

# Add a table of contents to your post.
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
toc:
  - name: Equations
  - name: Images and Figures
    subsections:
    - name: Interactive Figures
  - name: Citations
  - name: Footnotes
  - name: Code Blocks
  - name: Layouts
  - name: Other Typography?

# Below is an example of injecting additional post-specific styles.
# This is used in the 'Layouts' section of this post.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
  .small_img {
    width: 50%;
    height: auto;
    margin-left: auto;
    margin-right: auto;
  }
---

## Goal of this blog post  
NetAug TODO--cite caters on how to train small neural networks like MobileNetV2-Tiny for the best top-k percent accuracy. They argue that training small neural networks technically differs from that of large neural networks because the former are prone to underfitting. NetAug is contrary to traditional methods like dropout, network pruning, quantization, data augmentation and other regularization techniques and it can be viewed as a reversed form of dropout, as we enlarge the target model instead of shrinking it. In this blog post, we identify some problems with NetAug and propose potential workarounds. 

## Background  
NetAug solely focuses on how to efficiently train small neural networks with less memory compute and be able to deploy them on edge devices and overcome underfitting. It's certainly easy to train a huge neural network, the one most predominant issue that big models suffer from is overfitting. Several techniques like data augmentation,pruning,dropout,knowledge distillation have been proposed. 

1. **Knowledge Distillation**:  It is difficult to deploy and maintain an ensemble. However, previous research has shown that it is possible to learn a single cumbersome model that has the same performance as an ensemble. In most knowledge distillation methods there exist a large teacher model that transfers its knowledge as a learned mapping via training to a small student model with the teacher models output. There exists techniques like self-distillation in which the teacher model trains itself. Convectional KD methods try to optimise the objective function such that the loss function penalizes the difference between the student and teacher model.

2. **Regularization**: Regularization is used to prevent overfitting of any ML model by reducing the variance, penalizing the model coefficients and complexity of the model. Regularization techniques are mainly composed of data augmentation one of the most simplest and conveninet ways to expand the size of the dataset such that it prevents overfitting issues that occur with a relatively small dataset. Dropout is also popularly applied while training models, in which at every iteration incoming and outgoing connections between certain nodes are randomly dropped based on a particular probability and the remaining neural network is trained normally. 

3. **Tiny Deep learning**:  Several challenges are paved while transitioning from conventional high end ML systems to low level clients, maintaining the accuracy of learning models, provide train-to-deploy facility in resource economical tiny edge devices, optimizing processing capacity. This method includes AutoML procedures for designing automatic techniques for architecturing apt neural netowrks for a given target hardware platform includes customised fast trained models,auto channel pruning methods and auto mixed precision quantization. The other approaches like AutoAugment methods automatically searches for improvised data augmentation within the network to prevent overfitting. There exists network slimming methods to reduce model size, decrease the run-time memory footprint and computing resource. 

4. **Network Augmentation**: This method was proposed to solve the problem of underfitting in tiny neural networks. This is done by augmenting the given model (referred to as base model) into a larger model and encourage it to work as a sub-model to get extra supervision in additoin to functioning independently. This will help in increasing the representation power of the base model because of the gradient flow from the larger model and it can be viewed equivalently as "**reverse-dropout**". 

5. **Neural Architechture Search**: 

--- 
## The Problem With Network Augmentation a.k.a NetAug  

1. **Training the Generated Augmented Models**: NetAug randomly samples sub-models from the largest model by augmenting the width instead of depth, its highly important to ensure we speed up the training time and reduce the number of traning iteration for each of these generated sub-models thereby enhancing the training convergence and reaching optimization faster. We theoretical aim at aiding this by introducing a re-parametrisation training that involves sharing and unsharing of weights.

<!-- 
Rough idea for solution: The parameter s is the diversity factor, to control the number of augmented model configurations. Try to penalise/introduce a reward for s to favor a particular configurations that perform better/provide better auxiliary supervision.
 -->

2. **Naive Loss Function**: NetAug computes loss in a very trivial form, i.e, by simply summing over the loss of the base model with that from the respective sampled augmented models. IDK FILLL PLS

<!--3. **Overfitting of the Augmented Models**: As the network grows lineary even the number of augmented models increase, thereby overfitting during the training process. Having large number of such models with a poor representation among them will simply increase the training overhead and will cause haphazard confusion while selecting the largest augmented model. NetAug focuses on design and implementing a single best largest, going ahead with just the largest paramater model would yield in bad accuracy and will worsen the overall model performance.-->

4. **Generating the largest Augmented Model**: For a particular network, 

---

## Formulation of NetAug

The end goal of any ML model is to be able to minimize the loss function with the help of gradient descent. Since Tiny neural network have a very small capacity the gradient descent is likely to get stuck in local minima. NetAug encourages the tiny neural network to work as a sub-model of a set of larger models constructed by augmenting the width of the tiny model. The largest augmented model,is constructed by augmenting the width ie: width of the largest augmented convolution operation is $$r \times w$$, where $$w$$ is width of a convolution operation and $$r$$ is augmentation factor.
After building the largest augmented model, we construct other augmented models by selecting a subset of channels from the largest augmented model. We use a hyper-parameter $$s$$, named diversity factor, to control the number of augmented model configurations. We set the augmented widths to be linearly spaced between $$w$$ and $$r \times w$$. For instance, with $$r = 3$$ and $$s = 2$$, the possible widths are $$[w, 2w, 3w]$$.

### Introducing NetAug with Relation Knowledge Distribution(RKD): 

Knowledge in learned models is constituted of:  
    1. (IKD) Outputs of individual samples represented by the teacher  
    2. (RKD) Relation among examples represented by the teacher
    
  1. RKD is a generalization of convectional knowledge distillation that combines with NetAug to boost the performance due to its complementarity with conventional KD, that aims at transferring structural knowledge using mutual relations of data examples in the teacher’s output presentation rather than individual output themselves. Contrary to conventional approaches called as Individual KD (IKD) that transfers individual outputs of the teacher model (fT) to the student model (fS) point-wise, RKD transfers relations of the outputs structure-wise and computes a relational potential ψ for every n-tuple of data instance and transfers the relevant information through the potential from the teacher to the student models. In addition to knowledge represented in the output layers and the intermediate layers of a neural network, knowledge that captures the relationship between feature maps are also used to train a student model. 
 
  2. We specifically aim at training the teacher and student model in a online setting, in this online type of distillation training method/scheme is one in which both the teacher and the student model is trained simultaneously at the same instance of time. 

{% include figure.html path="assets/img/2023-01-15-TinyML/rkd.png" class="img-fluid" %}

  Relational Knowledge distillation can be expressed by:  
    $$\psi$$: function extracting relation  
    Given a teacher model T and a student model S, we let $$f(T)$$ and $$f(S)$$ be functions of the teacher and the student, respectively

  $$\begin{equation}
  \mathcal{L}_{\text{RKD}} = \sum \limits_{\{x_1, \ldots, x_n\} \in \chi^N} l \big(\psi(t_1,\cdots,t_n), \: \psi(s_1, \cdots, s_n)\big)
  \end{equation}$$

### RKD Losses
1. **Distance-wise distillation loss (pair)**:  

<div class="small_img row mt-1">
{% include figure.html path="assets/img/2023-01-15-TinyML/RKD-D.png" class="img-fluid" %}  
</div>

RKD-D transfer relative distance between points on embedding space.
  
  $$\begin{equation}\psi_{D}(t_i, t_j) = \frac{1}{\mu}\big\| t_i - t_j\big\|_2\end{equation}$$  

  where  $$\psi_d(\cdot, \cdot)$$ denotes distance wise potential function
    
  $$\begin{equation}\mu = \frac{1}{|\chi^2|}\sum\limits_{(x_i, x_j) \in \chi^2} \big\| t_i - t_j\big\|_2\end{equation}$$      
    
  $$\begin{equation}\boxed{\mathcal{L}_{\text{RKD-D}} = \sum \limits_{(x_i, x_j) \in \chi^2} l_\delta \big(\psi_D(t_i, t_j), \psi_D(s_i, s_j)\big)}\end{equation}$$   

  Where $$l_{\delta}$$ denotes the Huber Loss.
  
  $$\begin{equation}l_\delta(x, y) = \begin{cases}
                                \frac{1}{2} (x-y)^2\:\:\:\: \text{for} |x-y|^2 \\
                                |x - y| - \frac{1}{2}\:\:\: \text{otherwise.}
                                \end{cases}\end{equation}$$  


2. **Angle-wise distillation loss (triplet)**:  

<div class="small_img row mt-1">
{% include figure.html path="assets/img/2023-01-15-TinyML/RKD-A.png" class="img-fluid" %}  
</div>

RKD-A transfers angle formed by three points on embedding space.  
  
  $$\begin{equation}\psi_{A}(t_i, t_j, t_k) = \cos \angle t_it_jt_k = \langle \boldsymbol{e}^{ij}, \boldsymbol{e}^{jk}\rangle\end{equation}$$
    
  where  $$\psi_A(\cdot, \cdot, \cdot)$$ denotes angle wise potential function
  
  $$\begin{equation}\boldsymbol{e}^{ij} = \frac{t_i - t_j}{\big\|t_i - t_j\big\|_2}, \: \boldsymbol{e}^{jk} = \frac{t_k - t_j}{\big\|t_k - t_j\big\|_2}\end{equation}$$ 

  $$\begin{equation}\boxed{\mathcal{L}_{\text{RKD-A}} = \sum\limits_{(x_i, x_j, x_k) \in \chi^3} l_\delta \big(\psi_A(t_i, t_j, t_k), \psi_A(s_i, s_j, s_k)\big)}\end{equation}$$

---

### Combining RKD with NetAug:
    
$$\begin{equation}\mathcal{L}_{\text{aug}} = \underbrace{\mathcal{L}(W_t)}_{\text{base supervision}} \:+\: \underbrace{\alpha_1 \mathcal{L}([W_t, W_1]) + \cdots + \alpha_i \mathcal{L}([W_t, W_i]) + \cdots}_{\text{auxiliary supervision, working as a sub-model of augmented models}}  \:+\: \lambda_{\text{KD}}\,\underbrace{\mathbf{\mathcal{L}_{\text{RKD}}}}_{\text{relational knowledge distillation}}\label{eqn:loss_func}\end{equation}$$

where $$[W_t, W_i]$$ represents an augmented model where $$[W_t]$$ represents the tiny neural network and $$[W_i]$$ contains weight of the sub-model sampled from the largest augmented model, $$\alpha$$ is scaling hyper-parameter for combining loss from different augmented models and finally $$\lambda_{\text{KD}}$$ is a tunable hyperparameter to balance RKD and NetAug.

## Training

We train only one model for every epoch, training all the augmented models all once is not only computatioanlly expensive but also impacts the performance. The proportion of the base supervision will decrease when we sample more augmented networks, which will make the training process biased toward augmented networks and shadows the base TinyML model.

To further enhance the auxiliary supervision, we propose to use RKD in an online setting i.e., the largest augmented model will act as a teacher and the base model will act as a student. Both the teacher and the student are trained simultaneously.

We train the both the base model and the augmented model via gradient descent based on the loss function $$\eqref{eqn:loss_func}$$. The gradient update is then given by

$$\begin{equation}W^{n+1}_t = W^n_t - \eta \bigg(\frac{\partial \mathcal{L}(W^n_t)}{\partial W^n_t} + \alpha \frac{\partial \mathcal{L}([W^n_t, W^n_i])}{\partial W^n_t} + \lambda \frac{\partial \mathcal{L}_{\text{RKD}}([W^n_t, W^n_l])}{\partial W^n_t}\bigg)\end{equation}$$

---

## Fasten Auxillary Model Training

We propose to speed up the training process for the augmented sub models such that it can attain faster convergence and reduce the number of training iterations thereby obtain a better performance. In this technique, in the early phase of training, the neural network is trained with weights shared across all the layers of the model, to learn the commonly shared component across weights of different layers, towards the later phase of training we un-share weights and continue training until convergence. Weight sharing first trains the weights in a more constrained set. It brings the weights closer to the optimal value, which provides a better initialization for subsequent training steps and a better model generalization. 

**Algorithmic Motivation**  
Denote the neural model as consisting of $$L$$ modules as $$\mathcal{M} = \{\mathcal{M}_i\}, \, i=1,\cdots, L$$ and $$w = \{w_i\}, \, i=1,\cdots, L$$ denote the corresponding weights. These weights are re-parametrized as

$$\begin{equation}w_i = \frac{1}{\sqrt{L}}w_0 + \tilde{w}_i, \:\:\:\: i=1,\cdots,L\end{equation}$$  

Here $$w_0$$ represents the shared weights across all modules and is referred to as **stem-direction** and $$\tilde{w}_i$$ represents the unshared weights across all modules and is referred to as **branch-directions**.

**Training Strategy**  
Denote $$T$$ as the number of training steps, $$\eta$$ as the step size and $$\alpha \in (0, 1)$$ is a tunable hyper-paramter indicating the fraction of weight sharing steps. Then we train $$\mathcal{M}$$ as follows:
  - **Sharing weights in early stage:** For the first $$\tau = \alpha \cdot T$$ steps, we update the shared weights $$w_0$$ alone with gradient $$g_0$$
  - **Unsharing weights in later stage:** For the next $$t \geq \alpha \cdot T$$, we update only the unshared weights $$\tilde{w}_i$$ with gradient $$\tilde{g}_i$$
  
The effective gradient updates for $$w_i$$ can be found using chain rule as follows:

$$\begin{equation}g_0 = \frac{\partial \mathcal{L}}{\partial w_0} = \sum\limits_{i=1}^L \frac{\partial \mathcal{L}}{w_i}\,\frac{\partial w_i}{\partial w_0} = \frac{1}{\sqrt{L}}\sum\limits_{i=1}^L g_i
\end{equation}$$

$$\begin{equation}\tilde{g}_i = \frac{\partial \mathcal{L}}{\partial \tilde{w}_i} = \frac{\partial \mathcal{L}}{\partial w_i}\,\frac{\partial w_i}{\partial \tilde{w}_i} = g_i\end{equation}$$ 

where $$g_i$$ denotes the gradients of $$w_i$$ and $$\mathcal{L}$$ denotes the loss function.

{% include figure.html path="assets/img/2023-01-15-TinyML/NetAug_SWE.png" class="img-fluid" %}  


## Generating largest augmented model via Single Path One-Shot:

Single Path One-Shot revists the pitfalls of weight coupling in previous weight sharing methods. The one-shot paradigm is made attractive for real world tasks and better generalization. It is hyperparameter free, single path strategy works well because it can decouple the weights of different operations. This implementation can be more efficient in multiple search space. Using this technique we generate largest optimized augmented model by 

1. Supernet weight Optimization  <!-- $$\begin{equation} <> \end{equation}$$-->

$$ \begin{equation}W_{\mathcal{A}}=\mathop{\arg \min}_{W} \: \mathcal{L}_{\text{train}}\big(\mathcal{N}(\mathcal{A},W)\big)\end{equation}$$

  During an SGD step in the above equation, each edge in the supernet graph is randomly dropped, using a dropout rate parameter. In this way, the co-adaptation of the node weights is reduced during training making the supernet training easier.


2. Architecture Search Optimization

$$ \begin{equation}a^* = \mathop{\arg \max}_{a \in \mathcal{A}} \text{ACC}_{\text{val}}\bigg(\mathcal{N}\big(a,W_{\mathcal{A}}(a)\big)\bigg)\end{equation}$$

During search, each sampled architecture a inherits its weights from $$[W(A)]$$ as $$[W(A)a]$$. The architecture weights are ready to use making the search very efficient and flexible. The key to the success of architecture search in is that, the accuracy of any architecture a on a validation set using inherited weight $$[WA(a)]$$ (without extra fine tuning) is highly predictive for the accuracy of a that is fully trained. 

We try to minimize the training loss even further for better performance, supernet weights $$[W_A]$$ such that all the architectures in the search space are optimized simultaneously. 

$$\begin{equation}W_{\mathcal{A}} = \mathop{\arg \min}_{W} \: \mathbb{E}_{a \sim \Gamma(\mathcal{A})}\bigg[\mathcal{L}_{\text{train}}\big(\mathcal{N}(a, W(a))\big)\bigg]\end{equation}$$

The prior distribution $$\Gamma(\mathcal{A})$$ is important that is best found by uniform sampling, the distribution $$\Gamma(\mathcal{A})$$ is a fixed priori during our training and is not a learnable parameter.
    
## Evaluation and Inference
  
Evaluation metric is the core for building any accurate machine learning model. We propose to implement the evaluation metrics precision@k ,recall@k and f1-score@k for the augmented sub-models sampled from largest augmented model and for the base model itself, where k represents the top k accuracy on the test set. These metrics will assist in evaluating the training procedure better than vanilla accuracy because we are trying to tackle the problem of underfitting, and an underfitted model is more likely to make the same prediction for every input and presence of class imbalance will lead to erroneous results.

---

## Conclusion  

The main conclusion of this blog post is to further refine tiny neural networks effectively without any loss in accuracy and prevent underfitting in them. The paper implements this in a unique way apart from the conventional techniques such as regularization,dropout and data augmentation. This blog adds to other techniques that are orthogonal to NetAug can be used combined with NetAug for improvised results.



<!-- Notes/Overview/Category of the Paper, comment out later 
### Category
Solves a known problem of underfitting by proposing a new method

### Context
Regularization, over-fitting, underfitting, data augmentation, model augmentation, training paradigm, TinyML, knowledge distallation, security, privacy

### Correctness
Assumptions appear to be valid, but there are few setbacks (like eq 1 of the paper)

### Contributions
NetAug, Ablation study, improving the performance of underfitting models, new ways to consider loss functions

### Clarity
Well-written overall. Somethings are not clear-->

<!-- Questions/clarifications asked by us -->
<!--```markdown
Hi All, 

I hope everyone is doing well. I'm pursuing a BS from India and my friend is an employee. We came across this paper Network Augmentation for Tiny Deep Learning that was published in ICLR 22. We are unclear about a few things, would be great if you can clarify/justify them to us.

1. Why does sampling all augmented networks in one training step decrease the performance, given the training loss increases as one of the reasons?
2. Initially it's mentioned that NetAug is very different from the other knowledge distillation methods later the results show both NetAug and knowledge distillation combined together perform well, we would like to understand it more mathematically so as to why?
3. Things like data loading and communication cost are justified to consume maximum time, but don't we think we can spend training in doing something more potential like maybe in fact learning the feature maps/representations better?

Regards,
Nevasini.
```

```markdown
Hello Han Cai and team,
We are eager to get the above points clarified. Specifically, if you can answer along the following lines, it would be really helpful.
For 1 above: It was mentioned in the paper that it was found sampling more than one augmented network from the maintained single largest augmented model in each training step hurts the performance. We think this counters the intuition and spirit behind the idea of NetAug and the augmented loss proposed in equation (1) of the paper.
For 2 above: Can you further provide insights on why KD and NetAug can be complementary when the underlying motive in NetAug is to solve underfitting and in KD is to generalize well on unseem data with the help of a teacher? These both motives look like they want to solve the generalizability of the model under consideration. Furthermore, KD can be done to tiny models as well right?
For 3 above: We understand that data loading and communication c0st can be a bottleneck in training tiny neural networks. But in the case of NetAug, along with base supervision, we are training the augmented model as well (meaning we update the weights W_i). This means the training cost is expected to increase by 50% and experiments should also show a similar change.

Thanks and regards,
Nevasini and Ipsit.
```

```markdown-->
<!-- Their reply -->
<!--Hi Nevasini and Ipsit,

 

1. I think it is because the proportion of the base supervision will decrease when we sample more augmented networks, which will make the training process biased toward augmented networks.

 

2. I think the mechanisms of KD and NetAug are different. KD can be viewed as a better training objective with adaptive label smoothing. It does not encourage the base network to work as a subnetwork of larger networks, different from NetAug.

 

3. For tiny neural networks, the cost of the additional forward+backward is smaller than you expected. For example, training MobileNetV2-Tiny with NetAug on ImageNet only increases the training time by 16.7% on our GPU servers.

 

Best,

Han-->


